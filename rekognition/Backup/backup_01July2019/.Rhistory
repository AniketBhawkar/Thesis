coinModel <- round(runif(length(testing), min=0, max=1))
coinModel <- factor(coinModel, levels = c(0,1), labels = c("No", "Yes"))
coin[i] <- 1 - mean(coinModel != testing)
perish[i] <- 1 - mean(perishModel != testing)
}
results <- data.frame(coin, perish)
names(results) <- c("Coin Toss Accuracy", "Everyone Perishes Accuracy")
summary(results)
ggplot(melt(results), mapping = aes (fill = variable, x = value)) + geom_density (alpha = .5)
boxplot(results)
df <- win[, c("Win", "TossWin")]
df$Win <- factor(df$Win, levels = c(0,1), labels = c("No", "Yes"))
index <- sample(1:dim(df)[1], dim(df)[1] * .75, replace=FALSE)
training <- df[index, ]
testing <- df[-index, ]
table(training$Win, training$TossWin)
predictWin <- function(data) {
model <- rep("No", dim(data)[1])
model[data$TossWin == 1] <- "Yes"
return(model)
}
win <- c()
for (i in 1:1000) {
index <- sample(1:dim(df)[1], dim(df)[1] * .75, replace=FALSE)
testing <- df[-index, ]
winModel <- predictWin(testing)
win[i] <- 1 - mean(winModel != testing$Win)
}
results$`Winning Accuracy` <- win
names(results) <- c("Coin", "All Perish", "toss winning")
boxplot(results)
CrossTable(testing$Win, winModel)
CrossTable(testing$Win, winModel, prop.chisq = F, prop.c = F, prop.r = F)
confusionMatrix(table(testing$Win,winModel))
winModel1 = as.factor(winModel)
auc(testing$Win, winModel1)
library(caret)
library(e1071)
library(ModelMetrics)
library(dplyr)
library(ggplot2)
library(reshape)
library(gmodels)
#toss winning
win = read.csv("F:/Masters/Semester 2/Advanced Data Mining/IPL/raghu543-ipl-data-till-2017/clean datasets/Match_Impact_Teams.csv")
y <- win$Win
table(y)
y <- factor(y, levels = c(0,1), labels = c("No", "Yes"))
table(y)
prop.table(table(y))
barplot(table(y), main = "Distribution of winning team", ylab="Frequency")
set.seed(1337)
index <- sample(1:length(y), length(y) * .25, replace=FALSE)
testing <- y[index]
perishModel <- rep("No", length(testing))
coinModel <- round(runif(length(testing), min=0, max=1))
coinModel <- factor(coinModel, levels = c(0,1), labels = c("No", "Yes"))
perishModel <- factor(perishModel, levels = c("No", "Yes"), labels = c("No", "Yes"))
table(testing, perishModel)
table(testing, coinModel)
(coinAccuracy <- 1 - mean(coinModel != testing))
(perishAccuracy <- 1 - mean(perishModel != testing))
perish <- c()
coin <- c()
for (i in 1:1000) {
index <- sample(1:length(y), length(y) * .25, replace=FALSE)
testing <- y[index]
coinModel <- round(runif(length(testing), min=0, max=1))
coinModel <- factor(coinModel, levels = c(0,1), labels = c("No", "Yes"))
coin[i] <- 1 - mean(coinModel != testing)
perish[i] <- 1 - mean(perishModel != testing)
}
results <- data.frame(coin, perish)
names(results) <- c("Coin Toss Accuracy", "Everyone Perishes Accuracy")
summary(results)
ggplot(melt(results), mapping = aes (fill = variable, x = value)) + geom_density (alpha = .5)
boxplot(results)
df <- win[, c("Win", "TossWin")]
df$Win <- factor(df$Win, levels = c(0,1), labels = c("No", "Yes"))
index <- sample(1:dim(df)[1], dim(df)[1] * .75, replace=FALSE)
training <- df[index, ]
testing <- df[-index, ]
table(training$Win, training$TossWin)
predictWin <- function(data) {
model <- rep("No", dim(data)[1])
model[data$TossWin == 1] <- "Yes"
return(model)
}
win <- c()
for (i in 1:1000) {
index <- sample(1:dim(df)[1], dim(df)[1] * .75, replace=FALSE)
testing <- df[-index, ]
winModel <- predictWin(testing)
win[i] <- 1 - mean(winModel != testing$Win)
}
results$`Winning Accuracy` <- win
names(results) <- c("Coin", "All Perish", "toss winning")
boxplot(results)
CrossTable(testing$Win, winModel)
CrossTable(testing$Win, winModel, prop.chisq = F, prop.c = F, prop.r = F)
confusionMatrix(table(testing$Win,winModel))
winModel1 = as.factor(winModel)
auc(testing$Win, winModel1)
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
# Data
win = read.csv("F:/Masters/Semester 2/Advanced Data Mining/IPL/raghu543-ipl-data-till-2017/clean datasets/Match_Impact_Teams.csv")
str(win)
xtabs(~Win+Team_Id, data=win)
# Convert into Factor
win$Win<- as.factor(win$Win)
win$match_id <- as.factor(win$match_id)
win$Team_Id <- as.factor(win$Team_Id)
win$match_winner <- as.factor(win$match_winner)
# Visualization
pairs.panels(win[-8])
win %>%
ggplot(aes(x=Win, y=Avg_Impact_Fielding, fill = Win))+
geom_boxplot() +
ggtitle("Box Plot")
win %>%
ggplot(aes(x=Win, y=Avg_Impact_Batting, fill = Win))+
geom_boxplot() +
ggtitle("Box Plot")
win %>%
ggplot(aes(x=Win, y=Avg_Impact_Bowling, fill = Win))+
geom_boxplot() +
ggtitle("Box Plot")
win %>% ggplot(aes(x=Avg_Impact_Fielding, fill = Win))+
geom_density(alpha=0.8, color='black') +
ggtitle("Density Plot")
win %>% ggplot(aes(x=Avg_Impact_Batting, fill = Win))+
geom_density(alpha=0.8, color='black') +
ggtitle("Density Plot")
win %>% ggplot(aes(x=Avg_Impact_Bowling, fill = Win))+
geom_density(alpha=0.8, color='black') +
ggtitle("Density Plot")
# Data Partition
library(e1071)
set.seed(3696)
index <- sample(1:dim(win)[1],dim(win)[1]*.75,replace=FALSE)
training  <- win[index, ]
testing  <- win[-index, ]
############################# Naive Bayes' Model ##############################
model <- naive_bayes(Win ~ TossWin+Avg_Impact_Batting+Avg_Impact_Bowling+Avg_Impact_Fielding,data = training)
model
plot(model)
# Prediction
p <- predict(model, training, type = 'prob')
head(cbind(p,training))
# Confusion Matrix#
# Training Data
p1 <- predict(model,training)
(tab1 <- table(p1,training$Win))
# Misclassification
1-sum(diag(tab1)) / sum(tab1)
# Accuracy
sum(diag(tab1)) / sum(tab1)
# Testing Data
p2 <- predict(model,testing)
(tab2 <- table(p2,testing$Win))
## Misclassification
1-sum(diag(tab2)) / sum(tab2)
# Accuracy
sum(diag(tab2)) / sum(tab2)
win %>%
ggplot(aes(x=Win, y=Avg_Impact_Fielding, fill = Win))+
geom_boxplot() +
ggtitle("Box Plot")
win %>%
ggplot(aes(x=Win, y=Avg_Impact_Fielding.x, fill = Win))+
geom_boxplot() +
ggtitle("Box Plot")
win %>% ggplot(aes(x=Avg_Impact_Fielding.x, fill = Win))+
geom_density(alpha=0.8, color='black') +
ggtitle("Density Plot")
library(keras)
library(readr)
df = read.csv("F:/Masters/Semester 2/Advanced Data Mining/IPL/raghu543-ipl-data-till-2017/clean datasets/x10_over_2innings.csv")
df = df[c(-1,-2,-3,-4,-5,-6,-15,-16)]
data = as.matrix(df)
dimnames(data) = NULL
set.seed(1234)
index = sample(2,
nrow(data),
replace = TRUE,
prob = c(0.75,0.25))
x_train = data[index == 1,-10]
x_test = data[index == 2,-10]
#x_test_actual = data[index == 1,8]
y_test_actual = data[index == 2,10]
y_train = to_categorical(data[index==1,10])
y_test = to_categorical(data[index==2,10])
mean_train = apply(x_train,
2,
mean)
std_train = apply(x_train,
2,
sd)
x_train = scale(x_train,
center = mean_train,
scale = std_train)
x_test = scale(x_test,
center = mean_train,
scale = std_train)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = "relu", input_shape = c(9)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = "softmax")
summary(model)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = c("accuracy")
)
history <- model %>%
fit(x_train,
y_train,
epochs = 20,
batch_size = 128,
validation_split = 0.2)
plot(history)
model %>%
evaluate(x_test,
y_test)
pred = model %>%
predict_classes(x_test)
table(Predicted = pred,
Actual = y_test_actual)
prob = model %>%
predict_proba(x_test)
ans = cbind(prob,pred,y_test_actual)
library(keras)
library(readr)
df = read.csv("F:/Masters/Semester 2/Advanced Data Mining/IPL/raghu543-ipl-data-till-2017/clean datasets/x10_over_2innings.csv")
df = df[c(-1,-2,-3,-4,-5,-6,-15,-16)]
data = as.matrix(df)
dimnames(data) = NULL
set.seed(3696)
index = sample(2,
nrow(data),
replace = TRUE,
prob = c(0.75,0.25))
x_train = data[index == 1,-10]
x_test = data[index == 2,-10]
#x_test_actual = data[index == 1,8]
y_test_actual = data[index == 2,10]
y_train = to_categorical(data[index==1,10])
y_test = to_categorical(data[index==2,10])
mean_train = apply(x_train,
2,
mean)
std_train = apply(x_train,
2,
sd)
x_train = scale(x_train,
center = mean_train,
scale = std_train)
x_test = scale(x_test,
center = mean_train,
scale = std_train)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = "relu", input_shape = c(9)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = "softmax")
summary(model)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = c("accuracy")
)
history <- model %>%
fit(x_train,
y_train,
epochs = 20,
batch_size = 128,
validation_split = 0.2)
plot(history)
model %>%
evaluate(x_test,
y_test)
pred = model %>%
predict_classes(x_test)
table(Predicted = pred,
Actual = y_test_actual)
prob = model %>%
predict_proba(x_test)
ans = cbind(prob,pred,y_test_actual)
library(keras)
library(readr)
df = read.csv("F:/Masters/Semester 2/Advanced Data Mining/IPL/raghu543-ipl-data-till-2017/clean datasets/x15_over_2innings.csv")
df = df[c(-1,-2,-3,-4,-5,-6,-15,-16)]
data = as.matrix(df)
dimnames(data) = NULL
set.seed(3696)
index = sample(2,
nrow(data),
replace = TRUE,
prob = c(0.75,0.25))
x_train = data[index == 1,-10]
x_test = data[index == 2,-10]
#x_test_actual = data[index == 1,8]
y_test_actual = data[index == 2,10]
y_train = to_categorical(data[index==1,10])
y_test = to_categorical(data[index==2,10])
mean_train = apply(x_train,
2,
mean)
std_train = apply(x_train,
2,
sd)
x_train = scale(x_train,
center = mean_train,
scale = std_train)
x_test = scale(x_test,
center = mean_train,
scale = std_train)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = "relu", input_shape = c(9)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = "softmax")
summary(model)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = c("accuracy")
)
history <- model %>%
fit(x_train,
y_train,
epochs = 20,
batch_size = 128,
validation_split = 0.2)
plot(history)
model %>%
evaluate(x_test,
y_test)
pred = model %>%
predict_classes(x_test)
table(Predicted = pred,
Actual = y_test_actual)
prob = model %>%
predict_proba(x_test)
ans = cbind(prob,pred,y_test_actual)
setwd("F:/Masters/Semester 3/rekognition")
library(dplyr)
library(plyr)
library(tidyverse)
df = read.csv("./userProfiles.csv")
df1 = read.csv("./dataset-userCategory.csv")
setwd("F:/Masters/Semester 3/rekognition")
library(dplyr)
library(plyr)
library(tidyverse)
df = read.csv("./userProfiles.csv")
df1 = read.csv("./dataset-userCategory.csv")
View(df)
View(df1)
names(df1) <- c('id','Category')
View(df1)
df = df %>%
left_join(df1, by=c("id"))
write.csv(df, "./dataset2.csv",row.names = FALSE)
df2 = read.csv("./category.csv")
View(df2)
df2 = read.csv("./dataset1.csv")
View(df2)
View(df2)
df2 %>%
group_by(UserID) %>%
top_n(n=3, wt=id)
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=3, wt=id)
View(df3)
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=5, wt=id)
df3 = df3[,c(-1,-3)]
df4 = xtabs(UserID+Category, df3)
df4 =  dcast(df3, UserID~Category, fill=0)
library(reshape2)
df4 =  dcast(df3, UserID~Category, fill=0)
View(df4)
df4 =  dcast(df3, UserID~., fill=0)
View(df4)
View(df4)
df4 =  dcast(df3, UserID~Category, fill=0)
View(df4)
setwd("F:/Masters/Semester 3/rekognition")
library(dplyr)
library(plyr)
library(tidyverse)
library(reshape2)
df = read.csv("./userProfiles.csv")
df1 = read.csv("./dataset-userCategory.csv",header = FALSE)
df2 = read.csv("./dataset1.csv")
names(df1) <- c('id','MainCategory')
df = df %>%
left_join(df1, by=c("id"))
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=5, wt=id)
df3 = df3[,c(-1,-3)]
names(df3) <- c('id','Category')
df4 =  dcast(df3, id~Category, fill=0)
df = df4 %>%
left_join(df1, by=c("id"))
write.csv(df, "./dataset2.csv",row.names = FALSE)
View(df)
df = read.csv("./userProfiles.csv")
df1 = read.csv("./dataset-userCategory.csv",header = FALSE)
df2 = read.csv("./dataset1.csv")
names(df1) <- c('id','MainCategory')
df = df %>%
left_join(df1, by=c("id"))
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=5, wt=id)
df3 = df3[,c(-1,-3)]
names(df3) <- c('id','Category')
View(df3)
df4 =  dcast(df3, id~Category, fill=0)
View(df4)
setwd("F:/Masters/Semester 3/rekognition")
library(dplyr)
library(plyr)
library(tidyverse)
library(reshape2)
df = read.csv("./userProfiles.csv")
df1 = read.csv("./dataset-userCategory.csv",header = FALSE)
df2 = read.csv("./dataset1.csv")
names(df1) <- c('id','MainCategory')
df = df %>%
left_join(df1, by=c("id"))
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=5, wt=id)
df3 = df3[,c(-1,-3)]
setwd("F:/Masters/Semester 3/rekognition")
library(dplyr)
library(plyr)
library(tidyverse)
library(reshape2)
df = read.csv("./userProfiles.csv")
df1 = read.csv("./dataset-userCategory.csv",header = FALSE)
df2 = read.csv("./dataset1.csv")
names(df1) <- c('id','MainCategory')
df = df %>%
left_join(df1, by=c("id"))
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=5, wt=id)
df3 = df3[,c(-1,-3)]
View(df)
class(df$id)
class(df1$id)
View(df1)
df1 = read.csv("./dataset-userCategory.csv", header = FALSE)
View(df1)
df1 = read.csv("./dataset-userCategory.csv", header = FALSE)
View(df1)
names(df1) <- c('id','MainCategory')
df1 = read.csv("./dataset-userCategory.csv")
View(df1)
names(df1) <- c('id','MainCategory')
class(df1$id)
df = df %>%
left_join(df1, by=c("id"))
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=5, wt=id)
df3 = df3[,c(-1,-3)]
df4 =  dcast(df3, UserID~Category, fill=0)
View(df4)
View(df4)
colnames(df4)[1] <- "id"
df = df4 %>%
left_join(df, by=c("id"))
write.csv(df, "./dataset2.csv",row.names = FALSE)
setwd("F:/Masters/Semester 3/rekognition")
library(dplyr)
library(plyr)
library(tidyverse)
library(reshape2)
df = read.csv("./userProfiles.csv")
df1 = read.csv("./dataset-userCategory.csv")
df2 = read.csv("./dataset1.csv")
names(df1) <- c('id','MainCategory')
class(df1$id)
df = df %>%
left_join(df1, by=c("id"))
df3 = df2 %>%
group_by(UserID) %>%
top_n(n=5, wt=id)
df3 = df3[,c(-1,-3)]
df4 =  dcast(df3, UserID~Category, fill=0)
colnames(df4)[1] <- "id"
df = df %>%
left_join(df4, by=c("id"))
write.csv(df, "./dataset2.csv",row.names = FALSE)
